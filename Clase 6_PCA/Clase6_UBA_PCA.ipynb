{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnMR3sOW6j5y"
   },
   "source": [
    "# Big Data y Machine Learning (UBA) 2025\n",
    "## Clase 6 - Análisis de Componentes Principales (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFE_dv7D75Da"
   },
   "source": [
    "Componentes principales (PCA, en inglés) es una técnica de **aprendizaje no supervisado**. Es decir que nos encontramos en una situación donde tenemos información de un conjunto de variables o features ($X_1, X_2, ..., X_p$), pero no sobre una variable de resultado o outcome ($Y$). Vamos a tratar de ajustar algoritmos que interpreten la distribución de nuestros datos y encuentren relaciones interesantes entre éstos, trabajando con la naturaleza propia de los datos y sin un outcome de interés $Y$.  \n",
    "Esto se diferencia del **aprendizaje supervisado**, caso en el cual los estimadores se usan para **predecir** resultados basados en datos que poseen un outcome o variable de resultado $Y$ (puede ser una etiqueta -clasificación- o un valor -regresión-).\n",
    "\n",
    "Los algoritmos de aprendizaje no supervisado pueden ser muy útiles para casos en los que se busca **reducir la dimensionalidad**, por ejemplo cuando se busca visualizar datos de gran dimensionalidad o se busca crear un índice. PCA suele emplearse como parte del **análisis descriptivo y exploratorio de datos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que tenemos $n$ observaciones y $p$ variables y queremos visualizarlas como parte de una análisis exploratorio de los datos.\n",
    "\n",
    "Podríamos realizar gráficos de a 2 variables, pero serían muchos si $p$ es grande...\n",
    "Entonces vamos a buscar una representación de los datos en menos dimensiones (2 usualmente) que capture la mayor información posible.\n",
    "\n",
    "Las dimensiones serán combinaciones lineales de las $p$ variables que tienen la mayor varianza posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWiCdrSF_fqo"
   },
   "source": [
    "Vamos a trabajar con la librería [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instalamos los paquetes necesarios\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn\n",
    "!pip install ISLP\n",
    "\n",
    "# Alternativa\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install statsmodels\n",
    "#!{sys.executable} -m pip install scikit-learn\n",
    "#!{sys.executable} -m pip install ISLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ISLP\n",
    "from ISLP import load_data\n",
    "from statsmodels.datasets import get_rdataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descargar los archivos de esta clase en una carpeta y poner el directorio\n",
    "os.chdir(\"/Users/mnromero/Dropbox/COURSES/2025 - S1- Big Data y Machine Learning (UBA)/Clases/Clase 6_PCA\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar con una base de datos que provee el [libro ISLP](https://islp.readthedocs.io/en/latest/datasets/USArrests.html).\n",
    "\n",
    "##### Violent Crime Rates by US State\n",
    "Contiene información sobre arrestos por asaltos, asesinatos y violaciones cada 100.000 habitantes en 50 estados de Estados Unidos en 1973. También tiene información sobre el porcentaje de población viviendo en zonas urbanas.\n",
    "\n",
    "- Murder: Murder arrests (per 100,000)\n",
    "- Assault: Assault arrests (per 100,000)\n",
    "- Rape: Rape arrests (per 100,000)\n",
    "- UrbanPop: Percent urban population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Chequeamos la base de datos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrests = get_rdataset('USArrests').data\n",
    "print(arrests.shape)\n",
    "print(\"\\n\", arrests.info)\n",
    "print(\"\\n\", arrests.dtypes)\n",
    "print(\"\\n\", arrests.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadistica descriptiva (promedio) de las variables originales\n",
    "print(arrests.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Transformamos las variables\n",
    "Escalamos las variables antes de usar PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos el transformador, \n",
    "scaler = StandardScaler(with_std=True, with_mean=True) \n",
    "# Aplicamos fit_transform al DataFrame\n",
    "arrests_transformed = pd.DataFrame(scaler.fit_transform(arrests), columns=arrests.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos que tengan media 0 y desvio estandar 1\n",
    "print(\"Promedio luego de la transformación\\n\",arrests_transformed.mean()) # luego de la estandarización la media es cero\n",
    "print(\"Desvío estandár luego de la transformación\\n\",arrests_transformed.std()) # la desviación estandar es uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos\n",
    "print(arrests_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importante: Por definición, `PCA()` requiere centrar las variables para que tengan media cero pero las escalamos por eficiencia computacional y no alterar los pesos de las variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aplicamos PCA \n",
    "Estamos buscando maximizar la varianza de los predictores con la restricción de normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos el modelo\n",
    "pca = PCA()\n",
    "arrests_pca = pca.fit_transform(arrests_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Veamos los distintos elementos de dicho método no supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores/ Indices (z_im) donde i (filas) m (componentes)\n",
    "scores = arrests_pca\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadings vectors\n",
    "loading_vectors = pca.components_ # cada fila corresponde a un CP y cada columna, a una variable\n",
    "print(\"Loadings:\\n\", pca.components_)\n",
    "print(\"Loadings del CP1:\\n\",pca.components_[0]) \n",
    "pca.components_[0,0] #loadings del CP1 variable 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Recordemos la norma igual 1 de la suma de los ponderadores al cuadrado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notar que si tomamos los loadings/ponderadores del primer componente principal, por ejemplo:\n",
    "(0.53589947)**2+(0.58318363)**2+(0.27819087)**2+(0.5434309)**2 \n",
    "# La suma de sus cuadrados vemos que es igual a 1. Es la restricción que habíamos puesto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tarea probar que: $\\phi_1'*\\phi_2=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Grafico de Dispersión (*Biplot*)\n",
    "Un ventaja de este método no supervisado es enfocarnos en los primeros dos componentes que nos resumen gran parte de la información original de nuestra matriz X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = 0, 1 # Componentes\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5)) # creamos 1 subplot\n",
    "ax.scatter(scores[:,0], scores[:,1]) # graficamos los valores de los CP1 y CP2\n",
    "ax.set_xlabel('ComponentePrincipal%d' % (i+1))\n",
    "ax.set_ylabel('ComponentePrincipal%d' % (j+1))\n",
    "plt.title('Analisis de los primeros dos componentes principales')\n",
    "for k in range(pca.components_.shape[1]): # loop que itera por la cantidad de features\n",
    "    ax.arrow(0, 0, pca.components_[i,k], pca.components_[j,k]) # flecha desde el origen (0) a las coordenadas\n",
    "    ax.text(pca.components_[i,k], pca.components_[j,k], arrests.columns[k]) # al final de cada flecha, nombre de la variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot\n",
    "# Ajustes, extendemos longitud de las flechas e invertimos el eje y\n",
    "\n",
    "i, j = 0, 1 # Componentes\n",
    "\n",
    "scale_arrow = s_ = 2 # para extender la longitud de las flechas y que se vean mejor\n",
    "scores[:,1] *= -1\n",
    "pca.components_[1] *= -1 # gira el eje y (CP2)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.scatter(scores[:,0], scores[:,1]) \n",
    "ax.set_xlabel('ComponentePrincipal%d' % (i+1))\n",
    "ax.set_ylabel('ComponentePrincipal%d' % (j+1))\n",
    "plt.title('Analisis de los primeros dos componentes principales')\n",
    "for k in range(pca.components_.shape[1]):\n",
    "    ax.arrow(0, 0, s_*pca.components_[i,k], s_*pca.components_[j,k])\n",
    "    ax.text(s_*pca.components_[i,k], s_*pca.components_[j,k], arrests.columns[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Proporcion de la Varianza explicada\n",
    "Para entender cuánta información de la matriz original $X$ resume cada uno de los componentes, podemos calcular y visualizar la varianza de la matriz original $X$ explicada por cada uno de los componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % de la Varianza explicada por los componentes \n",
    "print(pca.explained_variance_ratio_) # CP1 explica el 62% de la varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4)) # 2 subplots uno al lado del otro\n",
    "ticks = np.arange(pca.n_components_)+1 # para crear ticks en el eje horizontal\n",
    "ax = axes[0]\n",
    "ax.plot(ticks, pca.explained_variance_ratio_ , marker='o')\n",
    "ax.set_xlabel('Componente principal');\n",
    "ax.set_ylabel('Prop. de la varianza explicada')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xticks(ticks)\n",
    "# capture suprime la visualización de la figura parcialmente terminada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = axes[1]\n",
    "ax.plot(ticks, pca.explained_variance_ratio_.cumsum(), marker='o') \n",
    "ax.set_xlabel('Componente principal')\n",
    "ax.set_ylabel('Suma acumulada de la varianza explicada')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xticks(ticks)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para decidir *qué número de componentes usar*, podemos consultar un scree plot que nos muestre la proporción de variable explicada para cada uno de los componentes y la variación en la varianza total explicada por el total de los componentes.\n",
    "\n",
    "Típicamente se elige la cantidad de componentes para la cual la proporción de la varianza explicada cae para cada componente principal adicional (cuando hay un codo en el scree plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar con un dataset de vinos (de scikit-learn). Contiene características de 178 vinos y a qué segmento de consumidores pertenecen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el dataset y breve exploración\n",
    "wine_data = pd.read_csv('Wine.csv')\n",
    "print(wine_data.shape)\n",
    "print(wine_data.dtypes)\n",
    "print(wine_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos datos entre X e Y (por ahora, haremos de cuenta que no contamos con Y)\n",
    "wine_features = wine_data.iloc[:, 0:13].values\n",
    "wine_customer_segment = wine_data.iloc[:, 13].values\n",
    "\n",
    "# Vemos las etiquetas posibles de customer segment\n",
    "wine_customer_segment_unique, counts = np.unique(wine_customer_segment, return_counts=True)\n",
    "for value, count in zip(wine_customer_segment_unique, counts):\n",
    "    print(f\"Value: {value}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento. Estandarizar las variables\n",
    "# Iniciar scaler y aplicarlo\n",
    "sc = StandardScaler()\n",
    "wine_features_transformed = sc.fit_transform(wine_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por qué estandarizamos? El análisis es sensible a la varianza de las variables originales y eso puede ocasionar problemas a la hora de elegir los CPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "wine_pca = pca.fit_transform(wine_features_transformed) # Obtenemos los scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "wine_scores = wine_pca\n",
    "\n",
    "# % de la Varianza explicada por los componentes\n",
    "print(\"Varianza explicada:\", pca.explained_variance_ratio_)\n",
    "# El primer componente principal explica el 36% de la varianza, mientras que el segundo, explica el 19%\n",
    "\n",
    "# Loading vectors\n",
    "loading_vectors = pca.components_ # cada fila corresponde a un CP y cada columna, a una variable\n",
    "print(\"Loadings:\\n\", pca.components_)\n",
    "print(\"Loadings del CP1:\\n\",pca.components_[0]) \n",
    "\n",
    "# Visualizamos features y loadings\n",
    "for i, loading_vector in enumerate(loading_vectors):\n",
    "    print(f\"\\nLoading Vector CP{i+1}:\")\n",
    "    for j, feature in enumerate(wine_data.columns[:-1]):\n",
    "        print(f\"{feature}: {round(loading_vector[j],3)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame para los componentes principales\n",
    "pca_df = pd.DataFrame(data=wine_pca, columns=['Componente_1', 'Componente_2'])\n",
    "\n",
    "# Añadir la variable objetivo al DataFrame de los componentes principales\n",
    "pca_df['Customer_Segment'] = wine_customer_segment\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los componentes\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(pca_df['Componente_1'], pca_df['Componente_2'], c=wine_data['Customer_Segment'], cmap='viridis')\n",
    "plt.xlabel('Componente Principal 1', fontsize=11)\n",
    "plt.ylabel('Componente Principal 2', fontsize=11)\n",
    "plt.title('Analisis de los primeros dos componentes principales')\n",
    "plt.colorbar(label='Customer Segment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Otra forma de graficar\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.xlabel('Componente Principal 1', fontsize=11)\n",
    "plt.ylabel('Componente Principal 2', fontsize=11)\n",
    "plt.title(\"Wine Data\",fontsize=16)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "\n",
    "targets = [1, 2, 3]\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indices_graf = pca_df['Customer_Segment'] == target\n",
    "    plt.scatter(pca_df.loc[indices_graf, 'Componente_1'], pca_df.loc[indices_graf, 'Componente_2'], c = color, s = 50)\n",
    "\n",
    "#plt.xlim(-4,4)\n",
    "#plt.ylim(-4,4)\n",
    "plt.legend(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La representación bidimensional de los datos tridimensionales capta correctamente el patrón principal de los datos: las observaciones rojas, azules y verdes, siguen estando en la representación bidimensional. \n",
    "\n",
    "Nota: aquí usamos dos componentes pero podríamos haber usado 1 o más de 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NFyrvNVL26HH",
    "pEiQnszy3lmi",
    "1yaJI88z4TfW",
    "ZoQVkU4C-GNd"
   ],
   "name": "Aprendizaje_no_supervisado.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "988c801e8fa6188d3e53012a7256361dd6100dad47899d4700f624e035bcb20b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
